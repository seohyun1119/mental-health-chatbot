{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 18.2 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-2.0.1-cp311-cp311-win_amd64.whl (172.3 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp311-cp311-win_amd64.whl (8.3 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.30.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp311-cp311-win_amd64.whl (143 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.5.5-cp311-cp311-win_amd64.whl (267 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, mpmath, tqdm, threadpoolctl, sympy, scipy, regex, pyyaml, networkx, joblib, fsspec, filelock, torch, scikit-learn, nltk, huggingface-hub, transformers, torchvision, sentence-transformers\n",
      "  Running setup.py install for sentence-transformers: started\n",
      "  Running setup.py install for sentence-transformers: finished with status 'done'\n",
      "Successfully installed filelock-3.12.0 fsspec-2023.5.0 huggingface-hub-0.14.1 joblib-1.2.0 mpmath-1.3.0 networkx-3.1 nltk-3.8.1 pyyaml-6.0 regex-2023.5.5 scikit-learn-1.2.2 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.1.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 tqdm-4.65.0 transformers-4.29.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sentence-transformers is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer  # 한국어 처리 모델\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 유사도 분석"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 로드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원천 데이터 (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile\n",
    "# 추출할 zip 파일 경로\n",
    "for i in range(1,5):\n",
    "    zip_file_path = f\"C:/dev/github/mental-health-chatbot/data/Training/TS_01. KAKAO({i}).zip\"\n",
    "    # 추출할 위치\n",
    "    extract_path = \"C:/dev/github/mental-health-chatbot/data/Training/data\"\n",
    "    # zipfile 객체 생성\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # 추출\n",
    "        zip_ref.extractall(extract_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m df_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m file_path \u001b[39min\u001b[39;00m file_paths:\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     14\u001b[0m         \u001b[39m# 각 파일의 내용을 DataFrame으로 변환\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 읽어올 파일들이 들어있는 폴더 경로\n",
    "folder_path = \"C:/dev/github/mental-health-chatbot/data/Training/data\"\n",
    "\n",
    "# 폴더 내의 모든 txt 파일 경로 리스트 생성\n",
    "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# 각 파일을 읽어와서 DataFrame으로 합치기\n",
    "i = 0\n",
    "df_list = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding = \"utf-8\") as f:\n",
    "        i += 1\n",
    "\n",
    "        # 각 파일의 내용을 DataFrame으로 변환\n",
    "        df = pd.read_csv(f, sep='\\t', header=None)\n",
    "        df['구분'] = i \n",
    "        df_list.append(df)\n",
    "\n",
    "# 모든 DataFrame 합치기\n",
    "result_df = pd.concat(df_list, ignore_index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>구분</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 : 애덜앙 나 너무 배불러서 배 아파</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 : 배 아프면 약국 가서 소화제라도 마시는 거 어때</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 : 얼마나 먹었다고 배가 아프데</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 : 떡볶이랑 튀김이랑 닭껍질튀김이랑 마카롱 먹었어</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 : 하긴 *이 배는 파리보다 작으니까 당연히 그만큼 먹어도 배부르지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2 : 그러니깐 ㅠ 이제 해리 자식들로 시작해야지</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1 : 근데 해리만큼 잘 할 수 있을까?</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2 : 진짜 그 3인방 케미가 최고였는데</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1 : 그치그치 완전 최고였는데 ㅜㅠ</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2 : 응 다시는 안 나올 조합이야</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190463 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0     구분\n",
       "0                    1 : 애덜앙 나 너무 배불러서 배 아파      1\n",
       "1            2 : 배 아프면 약국 가서 소화제라도 마시는 거 어때      1\n",
       "2                       3 : 얼마나 먹었다고 배가 아프데      1\n",
       "3             1 : 떡볶이랑 튀김이랑 닭껍질튀김이랑 마카롱 먹었어      1\n",
       "4   2 : 하긴 *이 배는 파리보다 작으니까 당연히 그만큼 먹어도 배부르지      1\n",
       "..                                      ...    ...\n",
       "11              2 : 그러니깐 ㅠ 이제 해리 자식들로 시작해야지  71651\n",
       "12                   1 : 근데 해리만큼 잘 할 수 있을까?  71651\n",
       "13                   2 : 진짜 그 3인방 케미가 최고였는데  71651\n",
       "14                     1 : 그치그치 완전 최고였는데 ㅜㅠ  71651\n",
       "15                      2 : 응 다시는 안 나올 조합이야  71651\n",
       "\n",
       "[1190463 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨링 데이터 (.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 추출할 zip 파일 경로\n",
    "for i in range(1,5):\n",
    "    zip_file_path = f\"C:/dev/github/mental-health-chatbot/data/Training/TL_01. KAKAO({i}).zip\"\n",
    "\n",
    "    # 추출할 위치\n",
    "    extract_path = \"C:/dev/github/mental-health-chatbot/data/Training/data_labeling\"\n",
    "\n",
    "    # zipfile 객체 생성\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # 추출\n",
    "        zip_ref.extractall(extract_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile\n",
    "\n",
    "# 추출할 zip 파일 경로\n",
    "zip_file_path = \"C:/dev/github/mental-health-chatbot/data/Validation/VL_01. KAKAO.zip\"\n",
    "\n",
    "# 추출할 위치\n",
    "extract_path = \"C:/dev/github/mental-health-chatbot/data/Validation/VL_01. KAKAO\"\n",
    "\n",
    "# zipfile 객체 생성\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\n",
    "    # 추출\n",
    "    zip_ref.extractall(extract_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 읽어올 파일들이 들어있는 폴더 경로\n",
    "folder_path = \"C:/dev/github/mental-health-chatbot/data/Training/data_labeling\"\n",
    "\n",
    "# 폴더 내의 모든 txt 파일 경로 리스트 생성\n",
    "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# 각 파일을 읽어와서 DataFrame으로 합치기\n",
    "i = 0\n",
    "df_list = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding = \"utf-8\") as f:\n",
    "        i += 1\n",
    "\n",
    "        # 각 파일의 내용을 DataFrame으로 변환\n",
    "        df = pd.read_csv(f, sep='\\t', header=None)\n",
    "        df['구분'] = i \n",
    "        df_list.append(df)\n",
    "\n",
    "# 모든 DataFrame 합치기\n",
    "result_df_json = pd.concat(df_list, ignore_index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>구분</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"dataset\": {</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"identifier\": 71459,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"name\": \"KAKAO_1000_01_set\",</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"src_path\": \"/data/file/cubeManager/PROJEC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>]</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>}</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>}</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>]</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>}</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16799461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0     구분\n",
       "0                                                    {      1\n",
       "1                                         \"dataset\": {      1\n",
       "2                                 \"identifier\": 71459,      1\n",
       "3                         \"name\": \"KAKAO_1000_01_set\",      1\n",
       "4        \"src_path\": \"/data/file/cubeManager/PROJEC...      1\n",
       "..                                                 ...    ...\n",
       "222                                                  ]  71651\n",
       "223                                                  }  71651\n",
       "224                                                  }  71651\n",
       "225                                                  ]  71651\n",
       "226                                                  }  71651\n",
       "\n",
       "[16799461 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24                 \"subject\": \"식음료\",\n",
       "24              \"subject\": \"주거와 생활\",\n",
       "24                  \"subject\": \"교통\",\n",
       "24            \"subject\": \"회사/아르바이트\",\n",
       "24                  \"subject\": \"군대\",\n",
       "                   ...              \n",
       "24                  \"subject\": \"미용\",\n",
       "24                  \"subject\": \"건강\",\n",
       "24              \"subject\": \"상거래 전반\",\n",
       "24               \"subject\": \"방송/연예\",\n",
       "24               \"subject\": \"영화/만화\",\n",
       "Name: 0, Length: 71651, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_json[0][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['        \"subject\": \"식음료\",', '        \"subject\": \"주거와 생활\",',\n",
       "       '        \"subject\": \"교통\",', ..., '        \"subject\": \"상거래 전반\",',\n",
       "       '        \"subject\": \"방송/연예\",', '        \"subject\": \"영화/만화\",'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_json[0][24].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>구분</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 : 애덜앙 나 너무 배불러서 배 아파</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 : 배 아프면 약국 가서 소화제라도 마시는 거 어때</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 : 얼마나 먹었다고 배가 아프데</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 : 떡볶이랑 튀김이랑 닭껍질튀김이랑 마카롱 먹었어</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 : 하긴 *이 배는 파리보다 작으니까 당연히 그만큼 먹어도 배부르지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2 : 그러니깐 ㅠ 이제 해리 자식들로 시작해야지</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1 : 근데 해리만큼 잘 할 수 있을까?</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2 : 진짜 그 3인방 케미가 최고였는데</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1 : 그치그치 완전 최고였는데 ㅜㅠ</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2 : 응 다시는 안 나올 조합이야</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190463 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0     구분\n",
       "0                    1 : 애덜앙 나 너무 배불러서 배 아파      1\n",
       "1            2 : 배 아프면 약국 가서 소화제라도 마시는 거 어때      1\n",
       "2                       3 : 얼마나 먹었다고 배가 아프데      1\n",
       "3             1 : 떡볶이랑 튀김이랑 닭껍질튀김이랑 마카롱 먹었어      1\n",
       "4   2 : 하긴 *이 배는 파리보다 작으니까 당연히 그만큼 먹어도 배부르지      1\n",
       "..                                      ...    ...\n",
       "11              2 : 그러니깐 ㅠ 이제 해리 자식들로 시작해야지  71651\n",
       "12                   1 : 근데 해리만큼 잘 할 수 있을까?  71651\n",
       "13                   2 : 진짜 그 3인방 케미가 최고였는데  71651\n",
       "14                     1 : 그치그치 완전 최고였는데 ㅜㅠ  71651\n",
       "15                      2 : 응 다시는 안 나올 조합이야  71651\n",
       "\n",
       "[1190463 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = result_df_json[0][24].str.split().str[-1]\n",
    "\n",
    "# new_state = [i : v  for i, v enumerate state]\n",
    "\n",
    "# result_df.구분 = new_state를 ndarray로 변경해서 result_df 구분 컬럼에 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24                 \"subject\": \"식음료\",\n",
       "24              \"subject\": \"주거와 생활\",\n",
       "24                  \"subject\": \"교통\",\n",
       "24            \"subject\": \"회사/아르바이트\",\n",
       "24                  \"subject\": \"군대\",\n",
       "                   ...              \n",
       "24                  \"subject\": \"미용\",\n",
       "24                  \"subject\": \"건강\",\n",
       "24              \"subject\": \"상거래 전반\",\n",
       "24               \"subject\": \"방송/연예\",\n",
       "24               \"subject\": \"영화/만화\",\n",
       "Name: 0, Length: 71651, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_json[0][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m group, data \u001b[39min\u001b[39;00m result_df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39m구분\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     s \u001b[39m=\u001b[39m result_df_json[\u001b[39m0\u001b[39m][\u001b[39m24\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     result_df\u001b[39m.\u001b[39;49mextend(s)\n\u001b[0;32m      5\u001b[0m result_df[\u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m result_df\n",
      "File \u001b[1;32mc:\\dev\\github\\pythonbasic\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "for group, data in result_df.groupby('구분'):\n",
    "    s = result_df_json[0][24]\n",
    "    result_df.extend(s)\n",
    "\n",
    "result_df['subject'] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
